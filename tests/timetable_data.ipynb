{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from BODSDataExtractor.extractor import TimetableExtractor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import requests\n",
    "from lxml import etree as et\n",
    "# from xml.etree import ElementTree as et\n",
    "from time import sleep\n",
    "from datetime import datetime, timedelta\n",
    "import xml.dom.minidom\n",
    "import xmltodict\n",
    "import json\n",
    "from bs4 import BeautifulSoup as bsoup\n",
    "import time\n",
    "import schedule\n",
    "import os\n",
    "import shutil\n",
    "import pytz\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# api key\n",
    "api_key = 'a3eda657579ef98d499ef515fbb5a32a86b22248'\n",
    "fb_noc = ['FBRI']\n",
    "org_name = 'First Bus'\n",
    "timetable_dataset_id_list = [5815, 5814, 5813, 2283]\n",
    "location_dataset_id_list = [5815, 5814, 5813, 2283]\n",
    "bus_list = ['70', '72', '74', 'm1', 'm3', 'm4']\n",
    "dataset_id = [2283, 2283, 5814, 5814]\n",
    "# dataset_id = 699\n",
    "latest_log = \"Log initiating...\\n\"\n",
    "output_dir = '../data/processed/timetable/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prettyprint(element, **kwargs):\n",
    "    xml = et.tostring(element, pretty_print=True, **kwargs)\n",
    "    print(xml.decode(), end='')\n",
    "\n",
    "def df_insert_row(original_df, row_dictionary):\n",
    "    # Convert row dictionary to DataFrame\n",
    "    row_df = pd.DataFrame([row_dictionary])\n",
    "    # Concatenate the original_df with the new row_df\n",
    "    updated_df = pd.concat([original_df, row_df], ignore_index=True)\n",
    "    return updated_df\n",
    "\n",
    "# For lxml.etree only\n",
    "def remove_namespace(response_content):\n",
    "    tree = et.fromstring(response_content)\n",
    "    # Remove annoying namespaces \n",
    "    for elem in tree.iter():\n",
    "        # print(elem.tag)\n",
    "        if '}' in elem.tag:\n",
    "            elem.tag = elem.tag.split('}', 1)[1]  # Strip namespace\n",
    "        # Remove namespace from attributes\n",
    "        attribs = elem.attrib\n",
    "        # print(elem.attrib)\n",
    "        for attrib in list(attribs.keys()):\n",
    "            if '}' in attrib:\n",
    "                new_attrib = attrib.split('}', 1)[1]\n",
    "                attribs[new_attrib] = attribs.pop(attrib)\n",
    "    et.cleanup_namespaces(tree)\n",
    "    return et.tostring(tree)\n",
    "\n",
    "def timetable_data_search(bus_list, api_key, org_name='', catalog_location='./docs/location_data_catalogue.csv'):\n",
    "    location_data_catalogue = pd.read_csv(catalog_location)\n",
    "    location_data_catalogue = location_data_catalogue[location_data_catalogue['Organisation Name'].str.contains(org_name)]\n",
    "    search_list = list(location_data_catalogue['Datafeed ID'])\n",
    "    feed_id_list = []\n",
    "    for feed_id in search_list:\n",
    "        response = requests.get(\"https://data.bus-data.dft.gov.uk/api/v1/datafeed/\"+str(feed_id)+\"/?api_key=\"+api_key)\n",
    "        bus_siri_file = et.fromstring(response.content)\n",
    "        buses = bus_siri_file.findall('.//{http://www.siri.org.uk/siri}LineRef')\n",
    "        unique_bus = sorted(list(set(str.lower(b.text) for b in buses)))\n",
    "        for bus in bus_list:\n",
    "            if unique_bus.count(str.lower(bus)) > 0:\n",
    "                feed_id_list.append((bus, feed_id))\n",
    "        sleep(0.1)\n",
    "    return feed_id_list\n",
    "\n",
    "def parse_runtime(duration):\n",
    "    # Define the regular expression pattern\n",
    "    pattern = re.compile(r'PT(?:(\\d+)M)?(?:(\\d+)S)?')\n",
    "    match = pattern.match(duration)\n",
    "    \n",
    "    if not match:\n",
    "        raise ValueError(f\"Invalid duration format: {duration}\")\n",
    "    \n",
    "    minutes = int(match.group(1)) if match.group(1) else 0\n",
    "    seconds = int(match.group(2)) if match.group(2) else 0\n",
    "    \n",
    "    total_seconds = minutes * 60 + seconds\n",
    "    return total_seconds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Get the dataset ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['74 70', 2283.0],\n",
       " ['72', 2283.0],\n",
       " ['174 173', 5813.0],\n",
       " ['172', 5813.0],\n",
       " ['m4', 5814.0],\n",
       " ['m1', 5814.0]]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feed_id_list = []\n",
    "pattern='|'.join(bus_list)\n",
    "timetable_data_catalogue = pd.read_csv('../docs/timetables_data_catalogue.csv')\n",
    "timetable_data_catalogue = timetable_data_catalogue[timetable_data_catalogue['Organisation Name'].str.contains(org_name, case=False, na=False)]\n",
    "timetable_data_catalogue = timetable_data_catalogue[timetable_data_catalogue['XML:National Operator Code'].isin(fb_noc)]\n",
    "timetable_data_catalogue = timetable_data_catalogue[timetable_data_catalogue['XML:Line Name'].str.contains(pattern, case=False, na=False)]\n",
    "timetable_data_catalogue = timetable_data_catalogue[['XML:Line Name', 'Data set ID']].values.tolist()\n",
    "timetable_data_catalogue"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Call the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "https://data.bus-data.dft.gov.uk/timetable/dataset/2283/download/\n",
      "https://data.bus-data.dft.gov.uk/timetable/dataset/2283/download/\n",
      "https://data.bus-data.dft.gov.uk/timetable/dataset/5814/download/\n",
      "https://data.bus-data.dft.gov.uk/timetable/dataset/5814/download/\n"
     ]
    }
   ],
   "source": [
    "for id in dataset_id:\n",
    "    timetable_raw = requests.get(\"https://data.bus-data.dft.gov.uk/api/v1/dataset/\"+str(id)+\"/?api_key=\"+api_key)\n",
    "    response_text = json.loads(timetable_raw.content)\n",
    "    print(response_text.get('url'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Process the Timetable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('../data/raw/timetable_70_74.xml', \"r\") as f:\n",
    "        raw_xml = f.read()\n",
    "timetable_data_xml = et.fromstring(remove_namespace(raw_xml))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get all stops and their coordinates, put in a table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_info = pd.DataFrame()\n",
    "for stop in timetable_data_xml.findall(\".//StopPoints/AnnotatedStopPointRef\"):\n",
    "    # print('package start')\n",
    "    tmp={}\n",
    "    for data in stop.iter():\n",
    "        if data.tag != 'AnnotatedStopPointRef':\n",
    "            # print('an attribute')\n",
    "            tmp[data.tag] = data.text\n",
    "            # print(f'{data.tag}:{data.text}')\n",
    "    stop_info = df_insert_row(stop_info, tmp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vehicle Journey"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "journey_info = pd.DataFrame()\n",
    "for journey in timetable_data_xml.findall(\".//VehicleJourneys/VehicleJourney\"):\n",
    "    # print('package start')\n",
    "    tmp={}\n",
    "    for data in journey.iter():\n",
    "        if data.text != None:\n",
    "            # print('an attribute')\n",
    "            tmp[data.tag] = data.text\n",
    "            # print(f'{data.tag}:{data.text}')\n",
    "    if journey.find('StartDeadRun/PositioningLink/RunTime') != None:\n",
    "        tmp['DeadRunRuntime'] = journey.find('StartDeadRun/PositioningLink/RunTime').text\n",
    "    journey_info = df_insert_row(journey_info, tmp)\n",
    "journey_info = journey_info[['LineRef','ServiceRef','JourneyPatternRef','BlockNumber','JourneyCode','VehicleJourneyCode','DepartureTime']].copy()\n",
    "\n",
    "# This version omit the multiple temporal data provided by multiple VehicleJourneyTimingLink tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Services"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "services_info = pd.DataFrame()\n",
    "for service in timetable_data_xml.findall(\".//Services/Service\"):\n",
    "    # print('package start')\n",
    "    for line in service.findall(\"Lines/Line\"):\n",
    "        tmp={}\n",
    "        tmp['LineId'] = line.attrib['id']\n",
    "        tmp['LineName'] = line.find('LineName').text\n",
    "        for out_attrib in line.find(\"OutboundDescription\"):\n",
    "            tmp['Direction'] = 'outbound'\n",
    "            for attrib in out_attrib.iter():\n",
    "                if attrib.text != None:\n",
    "                    tmp[attrib.tag] = attrib.text\n",
    "        services_info = df_insert_row(services_info, tmp)\n",
    "        tmp={}\n",
    "        tmp['LineId'] = line.attrib['id']\n",
    "        tmp['LineName'] = line.find('LineName').text\n",
    "        for in_attrib in line.find(\"InboundDescription\"):\n",
    "            tmp['Direction'] = 'inbound'\n",
    "            for attrib in in_attrib.iter():\n",
    "                if attrib.text != None:\n",
    "                    tmp[attrib.tag] = attrib.text\n",
    "        services_info = df_insert_row(services_info, tmp)\n",
    "\n",
    "services_info = services_info[['LineId','LineName','Direction','Origin','Destination','Description']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "std_services_info = pd.DataFrame()\n",
    "for journey in timetable_data_xml.findall(\".//Services/Service/StandardService/JourneyPattern\"):\n",
    "    # print('package start')\n",
    "    tmp={}\n",
    "    for attrib in journey.iter():\n",
    "        tmp['JourneyPatternId'] = journey.attrib['id']\n",
    "        if attrib.text != None:\n",
    "            tmp[attrib.tag] = attrib.text\n",
    "    std_services_info = df_insert_row(std_services_info, tmp)\n",
    "\n",
    "std_services_info = std_services_info[['JourneyPatternId','DestinationDisplay','Direction','RouteRef','JourneyPatternSectionRefs']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "service_lookup_table = (journey_info\n",
    "                        .merge(std_services_info, 'left', left_on=['JourneyPatternRef'], right_on=['JourneyPatternId'])\n",
    "                        .merge(services_info, 'left', left_on=['LineRef', 'Direction'], right_on=['LineId', 'Direction']))\n",
    "service_lookup_table = service_lookup_table[['LineId','ServiceRef','LineName','Origin','Destination','Description','Direction',\n",
    "                                             'JourneyPatternRef','BlockNumber','JourneyCode','VehicleJourneyCode','DepartureTime',\n",
    "                                             'JourneyPatternId','DestinationDisplay','RouteRef','JourneyPatternSectionRefs']].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Route Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "route_data = pd.DataFrame()\n",
    "for route in timetable_data_xml.findall(\".//Routes/Route\"):\n",
    "    # print(stop.attrib)\n",
    "    tmp={}\n",
    "    tmp['RouteId'] = route.attrib['id']\n",
    "    for data in route.iter():\n",
    "        if data.text != None:\n",
    "            # print('an attribute')\n",
    "            tmp[data.tag] = data.text\n",
    "            # print(f'{data.tag}:{data.text}')\n",
    "    route_data = df_insert_row(route_data, tmp)\n",
    "route_data = route_data[['RouteId','PrivateCode','Description','RouteSectionRef']].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "tracking_data = pd.DataFrame()\n",
    "for route in timetable_data_xml.findall(\".//RouteSections/RouteSection\"):\n",
    "    # print(stop.attrib)\n",
    "    tmp={}\n",
    "    # tmp['RouteSelectionRef'] = route.attrib['id']\n",
    "    for link in route.findall(\"RouteLink\"):\n",
    "        tmp['RouteSelectionRef'] = route.attrib['id']\n",
    "        tmp['RouteLinkRef'] = link.attrib['id']\n",
    "        tmp['StartPointRef'] = link.find(\"From/StopPointRef\").text\n",
    "        tmp['EndPointRef'] = link.find(\"To/StopPointRef\").text\n",
    "        tmp['Distance'] = link.find(\"Distance\").text\n",
    "        tmp['TrackingPoints'] = []\n",
    "        for point in link.findall(\"Track/Mapping/Location\"):\n",
    "            tmp['TrackingPoints'].append([float(point.find(\"Latitude\").text), float(point.find(\"Longitude\").text)])\n",
    "        tracking_data = df_insert_row(tracking_data, tmp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "timing_mapping_data = pd.DataFrame()\n",
    "for journey_pattern_selection in timetable_data_xml.findall(\".//JourneyPatternSections/JourneyPatternSection\"):\n",
    "    for timinglink in journey_pattern_selection.findall('JourneyPatternTimingLink'):\n",
    "        tmp={}\n",
    "        tmp['JourneyPatternSectionId'] = journey_pattern_selection.attrib['id']\n",
    "        tmp['JourneyPatternTimingLinkId'] = timinglink.attrib['id']\n",
    "        tmp['RouteLinkRef'] = timinglink.find('RouteLinkRef').text\n",
    "        timing_mapping_data = df_insert_row(timing_mapping_data, tmp)\n",
    "\n",
    "timing_data = pd.DataFrame()\n",
    "for leg_runtime in timetable_data_xml.findall(\".//VehicleJourneys/VehicleJourney\"):\n",
    "    # print(leg_runtime.find('VehicleJourneyCode').text)\n",
    "    for timinglink in leg_runtime.findall('VehicleJourneyTimingLink'):\n",
    "        tmp={}\n",
    "        # print(timinglink.attrib['id'])\n",
    "        tmp['VehicleJourneyTimingLinkId'] = timinglink.attrib['id']\n",
    "        tmp['JourneyPatternTimingLinkRef'] = timinglink.find('JourneyPatternTimingLinkRef').text\n",
    "        tmp['RunTime'] = parse_runtime(timinglink.find('RunTime').text)\n",
    "        if timinglink.find('.//WaitTime') != None:\n",
    "            tmp['WaitTime'] = parse_runtime(timinglink.find('.//WaitTime').text)\n",
    "        if leg_runtime.find('.//BlockNumber') != None:\n",
    "            tmp['BlockNumber'] = leg_runtime.find('.//BlockNumber').text\n",
    "        timing_data = df_insert_row(timing_data, tmp)\n",
    "\n",
    "# timing_data = timing_data.merge(timing_mapping_data, 'left', left_on=['JourneyPatternTimingLinkRef'], right_on=['JourneyPatternTimingLinkId']).copy()\n",
    "# timing_data = timing_data[['VehicleJourneyTimingLinkId','JourneyPatternTimingLinkRef','RouteLinkRef','RunTime']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Model Trips As GeoJSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list = ['../data/raw/timetable_m1.xml', '../data/raw/timetable_m3.xml', '../data/raw/timetable_m4.xml', '../data/raw/timetable_70_74.xml', '../data/raw/timetable_72.xml']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing ../data/raw/timetable_m1.xml\n",
      "\tGATHERING STOP INFORMATION\n",
      "\tEXTRACTING JOURNEY INFORMATION\n",
      "\tEXTRACTING SERVICE INFORMATION\n",
      "\tMERGING SERVICE INFORMATION\n",
      "\tEXTRACTING SPATIAL INFORMATION\n",
      "\tEXTRACTING TEMPORAL INFORMATION\n",
      "\tSAVE RESULTS FOR m1\n",
      "Processing ../data/raw/timetable_m3.xml\n",
      "\tGATHERING STOP INFORMATION\n",
      "\tEXTRACTING JOURNEY INFORMATION\n",
      "\tEXTRACTING SERVICE INFORMATION\n",
      "\tMERGING SERVICE INFORMATION\n",
      "\tEXTRACTING SPATIAL INFORMATION\n",
      "\tEXTRACTING TEMPORAL INFORMATION\n",
      "\tSAVE RESULTS FOR m3_m3x\n",
      "Processing ../data/raw/timetable_m4.xml\n",
      "\tGATHERING STOP INFORMATION\n",
      "\tEXTRACTING JOURNEY INFORMATION\n",
      "\tEXTRACTING SERVICE INFORMATION\n",
      "\tMERGING SERVICE INFORMATION\n",
      "\tEXTRACTING SPATIAL INFORMATION\n",
      "\tEXTRACTING TEMPORAL INFORMATION\n",
      "\tSAVE RESULTS FOR m4\n",
      "Processing ../data/raw/timetable_70_74.xml\n",
      "\tGATHERING STOP INFORMATION\n",
      "\tEXTRACTING JOURNEY INFORMATION\n",
      "\tEXTRACTING SERVICE INFORMATION\n",
      "\tMERGING SERVICE INFORMATION\n",
      "\tEXTRACTING SPATIAL INFORMATION\n",
      "\tEXTRACTING TEMPORAL INFORMATION\n",
      "\tSAVE RESULTS FOR 74_70\n",
      "Processing ../data/raw/timetable_72.xml\n",
      "\tGATHERING STOP INFORMATION\n",
      "\tEXTRACTING JOURNEY INFORMATION\n",
      "\tEXTRACTING SERVICE INFORMATION\n",
      "\tMERGING SERVICE INFORMATION\n",
      "\tEXTRACTING SPATIAL INFORMATION\n",
      "\tEXTRACTING TEMPORAL INFORMATION\n",
      "\tSAVE RESULTS FOR 72\n"
     ]
    }
   ],
   "source": [
    "master_service_lookup_table = pd.DataFrame()\n",
    "master_tracking_data = pd.DataFrame()\n",
    "master_timing_data = pd.DataFrame()\n",
    "master_stop_info = pd.DataFrame()\n",
    "\n",
    "for file in file_list:\n",
    "    print(f'Processing {file}')\n",
    "    with open(file, \"r\") as f:\n",
    "            raw_xml = f.read()\n",
    "    timetable_data_xml = et.fromstring(remove_namespace(raw_xml))\n",
    "\n",
    "    print('\\tGATHERING STOP INFORMATION')\n",
    "    # Get stop codes\n",
    "    stop_info = pd.DataFrame()\n",
    "    for stop in timetable_data_xml.findall(\".//StopPoints/AnnotatedStopPointRef\"):\n",
    "        # print('package start')\n",
    "        tmp={}\n",
    "        for data in stop.iter():\n",
    "            if data.tag != 'AnnotatedStopPointRef':\n",
    "                # print('an attribute')\n",
    "                tmp[data.tag] = data.text\n",
    "                # print(f'{data.tag}:{data.text}')\n",
    "        stop_info = df_insert_row(stop_info, tmp)\n",
    "\n",
    "    print('\\tEXTRACTING JOURNEY INFORMATION')\n",
    "    journey_info = pd.DataFrame()\n",
    "    for journey in timetable_data_xml.findall(\".//VehicleJourneys/VehicleJourney\"):\n",
    "        # print('package start')\n",
    "        tmp={}\n",
    "        for data in journey.iter():\n",
    "            if data.text != None:\n",
    "                # print('an attribute')\n",
    "                tmp[data.tag] = data.text\n",
    "                # print(f'{data.tag}:{data.text}')\n",
    "        if journey.find('StartDeadRun/PositioningLink/RunTime') != None:\n",
    "            tmp['DeadRunRuntime'] = journey.find('StartDeadRun/PositioningLink/RunTime').text\n",
    "        journey_info = df_insert_row(journey_info, tmp)\n",
    "    journey_info = journey_info[journey_info.columns.intersection(['LineRef','ServiceRef','JourneyPatternRef','BlockNumber','JourneyCode','VehicleJourneyCode','DepartureTime'])].copy()\n",
    "    # This version omit the multiple temporal data provided by multiple VehicleJourneyTimingLink tags\n",
    "\n",
    "    print('\\tEXTRACTING SERVICE INFORMATION')\n",
    "    services_info = pd.DataFrame()\n",
    "    for service in timetable_data_xml.findall(\".//Services/Service\"):\n",
    "        # print('package start')\n",
    "        for line in service.findall(\"Lines/Line\"):\n",
    "            tmp={}\n",
    "            tmp['LineId'] = line.attrib['id']\n",
    "            tmp['LineName'] = line.find('LineName').text\n",
    "            for out_attrib in line.find(\"OutboundDescription\"):\n",
    "                tmp['Direction'] = 'outbound'\n",
    "                for attrib in out_attrib.iter():\n",
    "                    if attrib.text != None:\n",
    "                        tmp[attrib.tag] = attrib.text\n",
    "            services_info = df_insert_row(services_info, tmp)\n",
    "            tmp={}\n",
    "            tmp['LineId'] = line.attrib['id']\n",
    "            tmp['LineName'] = line.find('LineName').text\n",
    "            for in_attrib in line.find(\"InboundDescription\"):\n",
    "                tmp['Direction'] = 'inbound'\n",
    "                for attrib in in_attrib.iter():\n",
    "                    if attrib.text != None:\n",
    "                        tmp[attrib.tag] = attrib.text\n",
    "            services_info = df_insert_row(services_info, tmp)\n",
    "    services_info = services_info[services_info.columns.intersection(['LineId','LineName','Direction','Origin','Destination','Description'])].copy()\n",
    "\n",
    "    std_services_info = pd.DataFrame()\n",
    "    for journey in timetable_data_xml.findall(\".//Services/Service/StandardService/JourneyPattern\"):\n",
    "        # print('package start')\n",
    "        tmp={}\n",
    "        for attrib in journey.iter():\n",
    "            tmp['JourneyPatternId'] = journey.attrib['id']\n",
    "            if attrib.text != None:\n",
    "                tmp[attrib.tag] = attrib.text\n",
    "        std_services_info = df_insert_row(std_services_info, tmp)\n",
    "    std_services_info = std_services_info[std_services_info.columns.intersection(['JourneyPatternId','DestinationDisplay','Direction','RouteRef','JourneyPatternSectionRefs'])].copy()\n",
    "\n",
    "    print('\\tMERGING SERVICE INFORMATION')\n",
    "    service_lookup_table = (journey_info\n",
    "                            .merge(std_services_info, 'left', left_on=['JourneyPatternRef'], right_on=['JourneyPatternId'])\n",
    "                            .merge(services_info, 'left', left_on=['LineRef', 'Direction'], right_on=['LineId', 'Direction']))\n",
    "    service_lookup_table['JourneyCode'] = service_lookup_table['JourneyCode'].astype(int).astype(str).str.zfill(4)\n",
    "    service_lookup_table = service_lookup_table[service_lookup_table.columns.intersection(['LineId','ServiceRef','LineName','Origin','Destination','Description','Direction',\n",
    "                                                'JourneyPatternRef','BlockNumber','JourneyCode','DepartureTime',\n",
    "                                                'JourneyPatternId','DestinationDisplay','RouteRef','JourneyPatternSectionRefs'])].copy()\n",
    "    \n",
    "    all_service_list = services_info['LineName'].unique().tolist()\n",
    "\n",
    "    print('\\tEXTRACTING SPATIAL INFORMATION')\n",
    "    route_data = pd.DataFrame()\n",
    "    for route in timetable_data_xml.findall(\".//Routes/Route\"):\n",
    "        # print(stop.attrib)\n",
    "        tmp={}\n",
    "        tmp['RouteId'] = route.attrib['id']\n",
    "        for data in route.iter():\n",
    "            if data.text != None:\n",
    "                # print('an attribute')\n",
    "                tmp[data.tag] = data.text\n",
    "                # print(f'{data.tag}:{data.text}')\n",
    "        route_data = df_insert_row(route_data, tmp)\n",
    "    route_data = route_data[route_data.columns.intersection(['RouteId','PrivateCode','Description','RouteSectionRef'])].copy()\n",
    "\n",
    "    tracking_data = pd.DataFrame()\n",
    "    for route in timetable_data_xml.findall(\".//RouteSections/RouteSection\"):\n",
    "        # print(stop.attrib)\n",
    "        tmp={}\n",
    "        # tmp['RouteSelectionRef'] = route.attrib['id']\n",
    "        for link in route.findall(\"RouteLink\"):\n",
    "            tmp['RouteSectionRef'] = route.attrib['id']\n",
    "            tmp['RouteLinkRef'] = link.attrib['id']\n",
    "            tmp['StartPointRef'] = link.find(\"From/StopPointRef\").text\n",
    "            tmp['EndPointRef'] = link.find(\"To/StopPointRef\").text\n",
    "            tmp['Distance'] = float(link.find(\"Distance\").text)\n",
    "            tmp['TrackingPoints'] = []\n",
    "            for point in link.findall(\"Track/Mapping/Location\"):\n",
    "                tmp['TrackingPoints'].append([float(point.find(\"Longitude\").text), float(point.find(\"Latitude\").text)])\n",
    "            tracking_data = df_insert_row(tracking_data, tmp)\n",
    "\n",
    "    tracking_data = tracking_data.merge(route_data[['RouteId', 'RouteSectionRef']], 'left', left_on=['RouteSectionRef'], right_on=['RouteSectionRef']).copy()\n",
    "    tracking_data = tracking_data[tracking_data.columns.intersection(['RouteId','RouteSectionRef','RouteLinkRef','StartPointRef','EndPointRef','Distance','TrackingPoints'])].copy()\n",
    "\n",
    "    service_count = len(all_service_list)\n",
    "    tracker_count = len(tracking_data)\n",
    "    tracking_data = pd.concat([tracking_data]*service_count, ignore_index=True)\n",
    "    tracking_data = pd.concat([tracking_data, pd.DataFrame({'LineRef':[line for line in all_service_list for _ in range(tracker_count)]})], axis=1)\n",
    "\n",
    "    print('\\tEXTRACTING TEMPORAL INFORMATION')\n",
    "    timing_mapping_data = pd.DataFrame()\n",
    "    for journey_pattern_selection in timetable_data_xml.findall(\".//JourneyPatternSections/JourneyPatternSection\"):\n",
    "        for timinglink in journey_pattern_selection.findall('JourneyPatternTimingLink'):\n",
    "            tmp={}\n",
    "            tmp['JourneyPatternSectionId'] = journey_pattern_selection.attrib['id']\n",
    "            tmp['JourneyPatternTimingLinkId'] = timinglink.attrib['id']\n",
    "            tmp['RouteLinkRef'] = timinglink.find('RouteLinkRef').text\n",
    "            timing_mapping_data = df_insert_row(timing_mapping_data, tmp)\n",
    "\n",
    "    timing_data = pd.DataFrame()\n",
    "    for leg_runtime in timetable_data_xml.findall(\".//VehicleJourneys/VehicleJourney\"):\n",
    "        # print(leg_runtime.find('VehicleJourneyCode').text)\n",
    "        for timinglink in leg_runtime.findall('VehicleJourneyTimingLink'):\n",
    "            tmp={}\n",
    "            # print(timinglink.attrib['id'])\n",
    "            tmp['VehicleJourneyTimingLinkId'] = timinglink.attrib['id']\n",
    "            tmp['JourneyPatternTimingLinkRef'] = timinglink.find('JourneyPatternTimingLinkRef').text\n",
    "            tmp['RunTime'] = parse_runtime(timinglink.find('RunTime').text)\n",
    "            if timinglink.find('.//WaitTime') != None:\n",
    "                tmp['WaitTime'] = parse_runtime(timinglink.find('.//WaitTime').text)\n",
    "            if leg_runtime.find('.//BlockNumber') != None:\n",
    "                tmp['BlockNumber'] = leg_runtime.find('.//BlockNumber').text\n",
    "            timing_data = df_insert_row(timing_data, tmp)\n",
    "\n",
    "    timing_data = timing_data.merge(timing_mapping_data, 'left', left_on=['JourneyPatternTimingLinkRef'], right_on=['JourneyPatternTimingLinkId']).copy()\n",
    "    timing_data = timing_data[timing_data.columns.intersection(['JourneyPatternSectionId','BlockNumber','JourneyPatternTimingLinkRef','RouteLinkRef','RunTime', 'WaitTime'])]\n",
    "\n",
    "    file_line_name = '_'.join(services_info['LineName'].unique())\n",
    "    print(f'\\tSAVE RESULTS FOR {file_line_name}')\n",
    "    \n",
    "    master_service_lookup_table = pd.concat([master_service_lookup_table, service_lookup_table], ignore_index=True)\n",
    "    master_tracking_data = pd.concat([master_tracking_data, tracking_data], ignore_index=True)\n",
    "    master_timing_data = pd.concat([master_timing_data, timing_data], ignore_index=True)\n",
    "    master_stop_info = pd.concat([master_stop_info, stop_info], ignore_index=True)\n",
    "        \n",
    "    \n",
    "    # service_lookup_table.to_csv(output_dir+'service_lookup_table_'+str(file_line_name)+'.csv',index=False)\n",
    "    # tracking_data.to_csv(output_dir+'tracking_data'+str(file_line_name)+'.csv',index=False)\n",
    "    # timing_data.to_csv(output_dir+'timing_data_'+str(file_line_name)+'.csv',index=False)\n",
    "    # stop_info.to_csv(output_dir+'stop_info_'+str(file_line_name)+'.csv',index=False)\n",
    "\n",
    "master_service_lookup_table.drop_duplicates().to_pickle(output_dir+'master_service_lookup_table.pkl')\n",
    "master_tracking_data.to_pickle(output_dir+'master_tracking_data.pkl')\n",
    "master_timing_data.drop_duplicates().to_pickle(output_dir+'master_timing_data.pkl')\n",
    "master_stop_info.drop_duplicates().to_pickle(output_dir+'master_stop_info.pkl')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
